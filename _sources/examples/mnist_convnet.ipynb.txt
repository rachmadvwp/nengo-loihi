{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional networks\n",
    "\n",
    "In this example we will show how to build and train\n",
    "a convolutional network in NengoDL,\n",
    "and then deploy that network on Loihi.\n",
    "\n",
    "We'll use the basic MNIST dataset to demonstrate the steps.\n",
    "The input data are images of handwritten digits,\n",
    "and the goal is for the network to classify each image as 0-9.\n",
    "\n",
    "We will assume here that the reader is somewhat familiar with NengoDL,\n",
    "and focus on the issue of how to use NengoDL to train a network for Loihi.\n",
    "For a more basic introduction to NengoDL, check out\n",
    "[the documentation](https://www.nengo.ai/nengo-dl)\n",
    "and [examples](https://www.nengo.ai/nengo-dl/examples).\n",
    "\n",
    "Note: this notebook requires that you do a [developer installation of NengoDL\n",
    "](https://www.nengo.ai/nengo-dl/installation.html#developer-installation)\n",
    "and switch to the `conv_transform` branch\n",
    "(`git checkout origin/conv_transform`).\n",
    "This is temporary, as the Loihi convolution implementation\n",
    "is currently a work in progress.\n",
    "It will soon be finalized and fully supported in the Nengo ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import requests\n",
    "    has_requests = True\n",
    "except ImportError:\n",
    "    has_requests = False\n",
    "\n",
    "import nengo_loihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for later\n",
    "def download(fname, drive_id):\n",
    "    \"\"\"Download a file from Google Drive.\n",
    "\n",
    "    Adapted from https://stackoverflow.com/a/39225039/1306923\n",
    "    \"\"\"\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    if os.path.exists(fname):\n",
    "        return\n",
    "    if not has_requests:\n",
    "        link = \"https://drive.google.com/open?id=%s\" % drive_id\n",
    "        raise RuntimeError(\n",
    "            \"Cannot find '%s'. Download the file from\\n  %s\\n\"\n",
    "            \"and place it in %s.\" % (fname, link, os.getcwd()))\n",
    "\n",
    "    url = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, params={'id': drive_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token is not None:\n",
    "        params = {'id': drive_id, 'confirm': token}\n",
    "        response = session.get(url, params=params, stream=True)\n",
    "    save_response_content(response, fname)\n",
    "\n",
    "\n",
    "# load mnist dataset\n",
    "if not os.path.exists('mnist.pkl.gz'):\n",
    "    urlretrieve('http://deeplearning.net/data/mnist/mnist.pkl.gz',\n",
    "                'mnist.pkl.gz')\n",
    "\n",
    "with gzip.open('mnist.pkl.gz') as f:\n",
    "    train_data, _, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "train_data = list(train_data)\n",
    "test_data = list(test_data)\n",
    "for data in (train_data, test_data):\n",
    "    one_hot = np.zeros((data[0].shape[0], 10))\n",
    "    one_hot[np.arange(data[0].shape[0]), data[1]] = 1\n",
    "    data[1] = one_hot\n",
    "\n",
    "# plot some examples\n",
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.reshape(train_data[0][i], (28, 28)))\n",
    "    plt.axis('off')\n",
    "    plt.title(str(np.argmax(train_data[1][i])));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by defining a simple function to build a \"convolutional layer\".\n",
    "This is just a `nengo.Connection` and `nengo.Ensemble` put together,\n",
    "but we'll be doing this a lot so we'll use this function\n",
    "to put them together in an easy-to-use bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, *args, activation=True, **kwargs):\n",
    "    # create a Conv2D transform with the given arguments\n",
    "    conv = nengo_loihi.Conv2D(*args, **kwargs)\n",
    "\n",
    "    if activation:\n",
    "        # add an ensemble to implement the activation function\n",
    "        layer = nengo.Ensemble(conv.output_shape.size, 1).neurons\n",
    "    else:\n",
    "        # no nonlinearity, so we just use a node\n",
    "        layer = nengo.Node(size_in=conv.output_shape.size)\n",
    "\n",
    "    # connect up the input object to the new layer\n",
    "    nengo.Connection(x, layer, transform=conv)\n",
    "\n",
    "    # print out the shape information for our new layer\n",
    "    print(\"LAYER\")\n",
    "    print(conv.input_shape.shape(), \"->\", conv.output_shape.shape())\n",
    "\n",
    "    return layer, conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the structure of our network.\n",
    "Because we need to keep the number of neurons and axons per core\n",
    "below the Loihi hardware limits,\n",
    "we adopt a somewhat unusual network architecture.\n",
    "We'll have a relatively small core network,\n",
    "so that each layer fits on one Loihi core,\n",
    "and then repeat that network several times in parallel, summing their output.\n",
    "We can think of this as a variation on [ensemble learning\n",
    "](https://en.wikipedia.org/wiki/Ensemble_averaging_(machine_learning))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.001  # simulation timestep\n",
    "presentation_time = 0.1  # input presentation time\n",
    "max_rate = 100  # neuron firing rates\n",
    "# neuron spike amplitude (scaled so that the overall output is ~1)\n",
    "amp = 1 / max_rate\n",
    "# input image shape\n",
    "input_shape = nengo_loihi.conv.ImageShape(28, 28, 1, channels_last=False)\n",
    "n_parallel = 2  # number of parallel network repetitions\n",
    "\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set up the default parameters for ensembles/connections\n",
    "    nengo_loihi.add_params(net)\n",
    "    net.config[nengo.Ensemble].neuron_type = (\n",
    "        nengo.SpikingRectifiedLinear(amplitude=amp))\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(\n",
    "        nengo.processes.PresentInput(test_data[0], presentation_time),\n",
    "        size_out=28 * 28)\n",
    "\n",
    "    # the output node provides the 10-dimensional classification\n",
    "    out = nengo.Node(size_in=10)\n",
    "\n",
    "    # build parallel copies of the network\n",
    "    for _ in range(n_parallel):     \n",
    "        layer, conv = conv_layer(\n",
    "            inp, 1, input_shape, kernel_size=1, kernel=np.ones((1, 1, 1, 1)))\n",
    "        # first layer is off-chip to translate the images into spikes\n",
    "        net.config[layer.ensemble].on_chip = False\n",
    "        layer, conv = conv_layer(layer, 6, conv.output_shape, strides=2)\n",
    "        layer, conv = conv_layer(layer, 24, conv.output_shape, strides=2)\n",
    "        nengo.Connection(layer, out, transform=nengo_dl.dists.Glorot())\n",
    "\n",
    "    out_p = nengo.Probe(out)\n",
    "    out_p_filt = nengo.Probe(out, synapse=nengo.Alpha(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to optimize the parameters of the network using NengoDL.\n",
    "\n",
    "First we set up the input/target data for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training data\n",
    "minibatch_size = 200\n",
    "train_inputs = {inp: train_data[0][:, None, :]}\n",
    "train_targets = {out_p: train_data[1][:, None, :]}\n",
    "\n",
    "# for the test data evaluation we'll be running the network over time\n",
    "# using spiking neurons, so we need to repeat the input/target data\n",
    "# for a number of timesteps (based on the presentation_time)\n",
    "test_inputs = {\n",
    "    inp: np.tile(test_data[0][:, None, :], (1, int(presentation_time / dt), 1))\n",
    "}\n",
    "test_targets = {\n",
    "    out_p_filt: np.tile(test_data[1][:, None, :],\n",
    "                        (1, int(presentation_time / dt), 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our error functions.\n",
    "We'll use two different error functions:\n",
    "classification error (the % of images classified incorrectly)\n",
    "as an intuitive measure of how well the network is doing,\n",
    "and crossentropy as\n",
    "the error measure the training processwill seek to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(outputs, targets):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=outputs, labels=targets))\n",
    "\n",
    "\n",
    "def classification_error(outputs, targets):\n",
    "    return 100 * tf.reduce_mean(\n",
    "        tf.cast(tf.not_equal(tf.argmax(outputs[:, -1], axis=-1),\n",
    "                             tf.argmax(targets[:, -1], axis=-1)),\n",
    "                tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the NengoDL simulator\n",
    "and run the training using the `sim.train` function.\n",
    "\n",
    "More details on how to use NengoDL to optimize a model\n",
    "can be found here: https://www.nengo.ai/nengo-dl/training.html.\n",
    "\n",
    "To speed up this example we can set `do_training=False`\n",
    "to load some pre-trained parameters.\n",
    "If you have the `requests` package installed,\n",
    "we will download these automatically.\n",
    "If not, download the following files\n",
    "to the directory containing this notebook.\n",
    "\n",
    "- [mnist_params.data-00000-of-00001](\n",
    "https://drive.google.com/open?id=18DAP42DL_XYgU_k1XiihBX9VqNLcV7XB)\n",
    "- [mnist_params.index](\n",
    "https://drive.google.com/open?id=1XkIMv71ylFByDlslE1LWh7cpYSvYu6yX)\n",
    "- [mnist_params.meta](\n",
    "https://drive.google.com/open?id=1WW4Md5aihwjuvsU3ISEeQHvSI6zEG2rP)\n",
    "\n",
    "Note that in order to run `do_training=True`,\n",
    "you will need to have TensorFlow installed with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=0) as sim:\n",
    "    if do_training:\n",
    "        print(\"error before training: %.2f%%\" %\n",
    "              sim.loss(test_inputs, test_targets, classification_error))\n",
    "\n",
    "        # run training\n",
    "        sim.train(train_inputs, train_targets,\n",
    "                  tf.train.RMSPropOptimizer(learning_rate=0.001),\n",
    "                  objective=crossentropy, n_epochs=5)\n",
    "\n",
    "        print(\"error after training: %.2f%%\" %\n",
    "              sim.loss(test_inputs, test_targets, classification_error))\n",
    "\n",
    "        sim.save_params(\"./mnist_params\")\n",
    "    else:\n",
    "        download(\"mnist_params.data-00000-of-00001\",\n",
    "                 \"18DAP42DL_XYgU_k1XiihBX9VqNLcV7XB\")\n",
    "        download(\"mnist_params.index\", \"1XkIMv71ylFByDlslE1LWh7cpYSvYu6yX\")\n",
    "        download(\"mnist_params.meta\", \"1WW4Md5aihwjuvsU3ISEeQHvSI6zEG2rP\")\n",
    "        sim.load_params(\"./mnist_params\")\n",
    "\n",
    "    # store trained parameters back into the network\n",
    "    sim.freeze_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we built it, the network has\n",
    "no synaptic filters on the neural connections.\n",
    "This works well during training,\n",
    "but we can see that the error is still somewhat high\n",
    "when we evaluate it using spiking neurons.\n",
    "We can improve performance\n",
    "by adding synaptic filters to our trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conn in net.all_connections:\n",
    "    conn.synapse = 0.005\n",
    "\n",
    "if do_training:\n",
    "    with nengo_dl.Simulator(net, minibatch_size=minibatch_size) as sim:\n",
    "        print(\"error w/ synapse: %.2f%%\" %\n",
    "              sim.loss(test_inputs, test_targets, classification_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our trained network, with synaptic filters, onto Loihi.\n",
    "This is as easy as passing the network to `nengo_loihi.Simulator`\n",
    "and running it, there is no extra work required.\n",
    "We will give the network 100 test images,\n",
    "and use that to evaluate the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_presentations = 100\n",
    "with nengo_loihi.Simulator(net, dt=dt, precompute=True) as sim:\n",
    "    # run the simulation on Loihi\n",
    "    sim.run(n_presentations * presentation_time)\n",
    "\n",
    "    # check classification error\n",
    "    step = int(presentation_time / dt)\n",
    "    output = sim.data[out_p_filt][step - 1::step]\n",
    "    correct = 100 * (np.mean(\n",
    "        np.argmax(output, axis=-1) \n",
    "        != np.argmax(test_data[1][:n_presentations], axis=-1)\n",
    "    ))\n",
    "    print(\"loihi error: %.2f%%\" % correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the output activity from the Loihi network\n",
    "as we show it different test images,\n",
    "to see what this performance looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = 10\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "images = test_data[0].reshape(-1, 28, 28, 1)\n",
    "ni, nj, nc = images[0].shape\n",
    "allimage = np.zeros((ni, nj * n_plots, nc), dtype=images.dtype)\n",
    "for i, image in enumerate(images[:n_plots]):\n",
    "    allimage[:, i * nj:(i + 1) * nj] = image\n",
    "if allimage.shape[-1] == 1:\n",
    "    allimage = allimage[:, :, 0]\n",
    "plt.imshow(allimage, aspect='auto', interpolation='none', cmap='gray')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(sim.trange()[:n_plots * step], sim.data[out_p_filt][:n_plots * step])\n",
    "plt.legend(['%d' % i for i in range(10)], loc='best');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
